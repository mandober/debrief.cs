# Coding theory

https://en.wikipedia.org/wiki/Coding_theory

**Coding theory** is the study of the properties of codes and their respective fitness for specific applications.

Codes are used for
- data compression
- cryptography
- error detection
- error correction
- data transmission
- data storage

Codes are studied by various scientific disciplines such as information theory, electrical engineering, mathematics, linguistics, and computer science.

Codes are studied for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

There are 4 types of coding:
- Data compression (or source coding)
- Error control (or channel coding)
- Cryptographic coding
- Line coding

*Data compression* attempts to remove redundancy in the source data in order to transmit it or store it more efficiently. For example, ZIP data compression makes data files smaller. Data compression and error correction are often studied in combination.

*Error correction* adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. A typical music compact disc uses the *Reed-Solomon code* to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the *turbo code* and *LDPC codes*.

## Contents

- 1. History of coding theory
- 2. Source coding
  - 2.1. Definition
  - 2.2. Properties
  - 2.3. Principle
  - 2.4. Example
- 3. Channel coding
  - 3.1. Linear codes
    - 3.1.1. Linear block codes
    - 3.1.2. Convolutional codes
- 4. Cryptographic coding
- 5. Line coding
- 6. Other applications of coding theory
  - 6.1. Group testing
  - 6.2. Analog coding
- 7. Neural coding


## History of coding theory

...

## Source coding

Main article: Data compression    
https://en.wikipedia.org/wiki/Data_compression

The aim of source coding is to take the source data and make it smaller.

### Definition

Data can be seen as a random variable `X : Œ© -> ùìß`, where `x ‚àà ùìß` appears with probability `‚Ñô‚Åü[X‚Åü=‚Åüx]`.

Data are encoded by strings (words) over an alphabet `Œ£`.

A code is a function `C : ùìß -> Œ£*` 
(or `Œ£+` if the empty string is not part of the alphabet).

`C(x)` is the code word associated with `x`.

Length of the code word is written as `l(C(x))`.

Expected length of a code is `l(C) = Œ£ {x ‚àà ùìß } l(C(x)) ‚Ñô‚Åü[X‚Åü=‚Åüx]`.

The concatenation of code words `C(x‚ÇÅ, ‚Ä¶, x‚Çñ) = C(x‚ÇÅ)‚ÅüC(x‚ÇÇ)‚Åü‚Ä¶‚ÅüC(x‚Çñ)`.

The code word of the empty string is the empty string itself, `C(œµ) = œµ`.

### Properties

1. `C : ùìß -> Œ£*`  is non-singular if injective.
2. `C : ùìß* -> Œ£*` is uniquely decodable if injective.
3. `C : ùìß -> Œ£*`  is instantaneous if `C(x‚ÇÅ)` is not a prefix of `C(x‚ÇÇ)` (and vice versa)

A code is **non-singular** if each source symbol is mapped to a different non-empty bit string, i.e. the mapping from source symbols to bit strings is injective.
